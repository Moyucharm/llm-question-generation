下面给你一份“本科毕设工作量可控、但内容足够完整”的 **LLM 智能出题 + 在线考试系统**总体规划。会结合你给的参考前端项目（QuAIz / Moyucharm/123）来判断“继续做还是重头做”，并给出推荐的技术栈、模块拆分、里程碑计划、论文与答辩 PPT 结构、演示脚本和风险降级方案。

---

# 一、毕设的具体要求
## 1. 你的毕设应该做到什么程度（边界先定死，避免越做越大）

### 必做（本科标准 + 够展示）

1. **教师端（Teacher）**
    
    - 课程/章节/知识点维护（最简可手动录入，进阶可 CSV 导入）
        
    - 配置出题（题型、数量、难度、知识点覆盖、分值）
        
    - 生成试卷（LLM）
        
    - **题目质量检查/自检流程**（LLM 自审 + 规则校验 + 教师人工确认）
        
    - 发布考试（时间限制、可见范围：全体/班级/指定学生）
        
    - 查看成绩与统计（班级整体、知识点薄弱点）
        
2. **学生端（Student）**
    
    - 参加考试 / 刷题（两种模式二选一也行，但建议都做：展示更完整）
        
    - 作答过程：计时、自动保存、交卷
        
    - 查看批改结果（客观题自动判分 + 主观题 LLM 批改反馈）
        
    - 错题本/历史记录
        
    - **个人薄弱点统计**（按知识点正确率/得分率）
        
3. **系统侧（后台）**
    
    - 登录鉴权与角色（教师/学生）
        
    - 数据库存储（用户、题库、试卷、考试、作答记录、批改结果、统计）
        
    - LLM 调用封装（仅 API，不做本地部署，不做微调）
        
    - 日志与审计（至少记录：调用时间、耗时、请求类型、模型、状态码/错误）
        

### 可选加分（不做也不影响本科通过）

- 多模型复核：生成用 A 模型、审题/批改用 B 模型（符合你“可用不同模型自我审查”的方向）
    
- 题目去重/相似题检测（轻量做法：同知识点下用哈希/关键词；重做法：向量检索，不建议本科搞）
    
- 导出试卷 PDF / Word（属于展示加分项，但不是核心）
    

### 明确不做（避免难度爆炸）

- 模型本地部署、微调、训练（你已明确不做）
    
- 代码题在线判题沙箱（太容易变成另一个毕设）
    
- 强防作弊（摄像头、屏幕监控等）——最多做简单“切屏提醒/全屏”即可
    

---

## 2. 参考项目要不要继续做？（结论：前端沿用 + 后端重建最划算）

你给的参考项目（QuAIz / Moyucharm/123）本质是一个**纯前端的 AI 出题 + 答题 + 批改应用**，技术栈是 **React 19 + TypeScript + Vite + TailwindCSS**，并且已经实现了多题型、流式生成、在线答题与 AI 批改、日志面板、时间记录等“展示效果很强”的能力。([GitHub](https://github.com/Moyucharm/123 "GitHub - Moyucharm/123"))

但它是“课程设计型”的纯前端方案，一旦进入毕设场景，会有天然短板：

- **不能安全保存/管理模型 API Key**（前端直连风险大）
    
- **没有真实的用户/角色/权限体系**
    
- **没有数据库**，无法做“历史学习统计、薄弱点统计、题库管理、考试发布”等你毕设核心方向
    

### 推荐方案（最适合你）

**不要重头做 UI**，而是：

> **前端：基于参考项目继续改**（保留答题体验、日志、计时等亮点）  
> **后端：从 0 搭建**（鉴权、数据库、出题/批改/统计 API、LLM 调用都放后端）

这样做的好处：

- 工作量可控：你不需要从零做复杂交互页面
    
- 你的“原创工作”主要体现在后端架构、数据库设计、质量控制流程、统计分析，这些很符合毕设
    
- 最终系统形态符合“前后端分离 + 数据库”的正规工程范式（答辩老师也更认可）
    

### 什么时候才建议重头做？

只有一种情况：参考项目前端结构你完全看不懂、改不动、越改越乱。  
但从 README 看它模块化做得挺细，反而更适合作“前端底座”。([GitHub](https://github.com/Moyucharm/123 "GitHub - Moyucharm/123"))

---

## 3. 技术栈推荐（按“好实现 + 好部署 + 本科难度”来选）

你说“没有擅长的”，那我给你选一套**学习曲线平缓、资料多、部署简单**的组合：

### 前端（沿用参考项目思路）

- React + TypeScript + Vite
    
- UI：TailwindCSS（参考项目就是这个路线）([GitHub](https://github.com/Moyucharm/123 "GitHub - Moyucharm/123"))
    
- 状态管理：Zustand（参考项目用的也是它）([GitHub](https://github.com/Moyucharm/123 "GitHub - Moyucharm/123"))
    
- 网络：fetch/axios 均可
    
- 流式输出：SSE（Server-Sent Events）最简单，前端也好接
    

### 后端（推荐 Python FastAPI）

- **FastAPI**：写 API 很快，自带 Swagger 文档，Pydantic 校验非常适合做“题目结构化 JSON 校验”
    
- ORM：SQLModel 或 SQLAlchemy（都行）
    
- 认证：JWT（访问令牌）
    
- 密码：bcrypt
    
- 任务：先用 FastAPI BackgroundTasks（够本科），想稳一点再上 Celery+Redis（可选）
    

### 数据库

- **SQLite：完全可行**（尤其适合本科 + 单机部署 + 开发方便）
    
- 建议开启 WAL 模式提升并发写性能（实现也不难）
    
- 如果你后期想“云端更稳”，可以无痛切 PostgreSQL（用 ORM 迁移即可），但不是必须
    

### 部署（云服务器）

- Docker + docker-compose
    
- Nginx 反代：
    
    - `/` -> 前端静态文件
        
    - `/api` -> 后端 FastAPI
        
    - `/api/stream` -> SSE
        
- SQLite 文件挂载到 volume 做持久化
    

---

## 4. 系统总体架构（答辩时一张图就能讲清楚）

**前后端分离 + LLM 服务封装 + DB 存储**：

- **前端 Web**
    
    - 教师端：出题配置、题目预览/编辑、发布考试、成绩统计
        
    - 学生端：考试/练习、作答、结果反馈、错题本、个人统计
        
- **后端 API（FastAPI）**
    
    - Auth：注册/登录/角色鉴权
        
    - Exam：考试发布、开始、保存答案、交卷
        
    - QuestionBank：题库、知识点、试卷管理
        
    - AI Service：出题、审题、批改、学习建议（统一封装国内模型 API）
        
    - Analytics：薄弱点、趋势、班级分布
        
    - Logs：LLM 调用日志与错误日志
        
- **Database（SQLite）**
    
    - 存：用户、知识点、题目、试卷、考试、作答、评分、统计快照、LLM日志
        

---

## 5. 功能模块拆解（做到这里，毕设就很完整了）

### 5.1 教师端功能（建议做到“能闭环”）

1. **课程/知识点管理**
    
    - 课程（Course）-> 章节（Chapter）-> 知识点（KnowledgePoint）
        
    - 最简：教师手动录入 + 维护
        
    - 进阶：CSV 导入知识点（加分但不难）
        
2. **AI 出题配置**
    
    - 学科/课程
        
    - 知识点范围（必选/可选）
        
    - 难度：简单/中等/困难（或 1~5）
        
    - 题型：单选/多选/填空/简答（先做这四类最稳）
        
    - 数量、总分、每题分值
        
    - “出题风格”可选项：偏概念/偏计算/偏应用（可选）
        
3. **题目生成与审题发布流程（核心亮点）**
    
    - 生成（LLM）
        
    - **结构化校验（JSON Schema + 规则校验）**
        
    - **AI 自审/复核**（同模型自检或不同模型复核）
        
    - 教师人工预览：
        
        - 一键“通过/驳回/局部重写/重新生成某一题”
            
    - 入库到题库
        
    - 组卷/发布考试
        
4. **成绩与统计**
    
    - 考试成绩列表、导出（可选）
        
    - 班级/学生：知识点正确率排行、薄弱点 TopN
        
    - 用时统计（考试平均耗时、题目平均耗时）
        

---

### 5.2 学生端功能（展示体验要好）

1. **考试/练习入口**
    
    - 考试：有时间限制、一次作答、交卷后出结果
        
    - 练习：可以反复刷、错题回顾
        
2. **作答体验**
    
    - 题目导航（答了/未答）
        
    - 自动保存（每次修改答案就 POST 保存）
        
    - 计时器（剩余时间/用时）
        
    - 交卷确认
        
3. **结果与学习报告**
    
    - 客观题：立刻判分
        
    - 主观题：展示 AI 批改建议（得分点、扣分点、改进建议）
        
    - 个人统计：知识点薄弱点、近期趋势（折线图可选）
        

---

## 6. 你“优化出题质量 + 自审正确性”的核心设计（毕设写作的主线）

你截图里列的 5 个“拟解决问题”非常适合直接变成论文的“关键技术点”。我把它们落成可实现的方案（不需要训练模型）：

### 6.1 题目质量控制（最关键）

**问题**：LLM 会事实错误、重复题、答案不唯一等。  
**方案（建议做成 3 段式流水线）**：

1. **生成阶段（Generator）**
    

- 强约束输出为 JSON（包含：题干、选项、标准答案、解析、知识点、难度、分值）
    
- 在 prompt 中明确：不得出现“可能/大概”；单选只能一个正确选项；解析必须能推导出答案
    

2. **规则校验（Validator，代码做）**
    

- JSON 解析成功？
    
- 单选：答案是否在选项内且唯一
    
- 多选：答案集合是否非空且都在选项内
    
- 填空：空数量是否匹配答案数量
    
- 简答：是否提供评分要点（rubric）
    
- 同一试卷内题目去重：题干哈希/相似度（先做“完全重复”检测即可）
    

3. **自审/复核（Reviewer，LLM 做）**
    

- 把“题目 + 标准答案 + 解析 + 知识点 + 难度”交给 Reviewer 模型
    
- 让它输出：
    
    - `is_correct`: true/false
        
    - `issues`: 列出问题类型（事实错误/答案不唯一/表述歧义/难度不符合…）
        
    - `fixed_question`: 若有问题，给出修正版 JSON
        
- 如果 reviewer 判定 false：
    
    - 自动走“修复一次”，仍失败则标记为“待人工审核”，不给学生直接使用
        

> 这套流程就是你毕设最核心的“优化出题质量”的落地实现：不用训练模型，但工程效果明显、论文也很好写。

---

### 6.2 题目结构化与后处理

**问题**：自然语言很难直接入库、难统一题型字段。  
**方案**：统一题目 JSON Schema（后端 Pydantic 模型）+ 自动纠错：

- 生成 prompt 强制输出 JSON
    
- 后端解析失败时：
    
    - 走一次“格式修复 prompt”（只修 JSON，不改内容）
        
- 入库前统一字段：`type / stem / options / answer / explanation / knowledge_point / difficulty / score`
    

---

### 6.3 难度与知识点可控

**问题**：生成难度飘、知识点覆盖不准。  
**方案（可做得很轻量）**

- 教师选择知识点列表，并在 prompt 中硬约束：**每题必须标注属于哪个知识点**
    
- 难度用 1~5 或 简中难，并给模型“难度描述准则”：
    
    - 简单：定义/记忆
        
    - 中等：理解+简单应用
        
    - 困难：综合/易混点/多步骤推理
        
- Reviewer 额外输出 `difficulty_estimate`，如果偏差大：
    
    - 自动改写（或提示教师一键调整）
        

---

### 6.4 性能与稳定性（本科别搞太重）

**问题**：LLM 延迟大、偶发超时。  
**建议实现（轻量但够用）**

- 生成与批改接口支持 **SSE 流式**（体验很好）
    
- 后端设置超时与重试（例如 1 次重试）
    
- 把“LLM 调用日志”落库：耗时、成功/失败、错误信息（方便论文写“系统可观测性”）
    
- 生成失败则返回“降级提示”：建议稍后重试或改用备用模型
    

---

### 6.5 用户体验与交互流程

你参考项目前端已经有“流式生成、答题、批改、日志、计时”等优秀体验点。([GitHub](https://github.com/Moyucharm/123 "GitHub - Moyucharm/123"))  
你的改造重点是：

- 老师端把“配置-生成-审核-发布”变成一条清晰主线
    
- 学生端把“开始考试-自动保存-交卷-看报告-看薄弱点”闭环做好
    

---

## 7. 数据库设计（SQLite 版本，够写 ER 图 + 够统计）

### 核心表（建议）

1. `users`：id, name, email, password_hash, role(student/teacher), created_at
    
2. `courses`：id, name, teacher_id
    
3. `knowledge_points`：id, course_id, name, chapter, parent_id（可选层级）
    
4. `questions`：id, course_id, type, stem, options(json), answer(json), explanation, difficulty, kp_id, created_by, status(draft/approved), created_at
    
5. `papers`（试卷模板/练习集）：id, course_id, title, total_score, created_by
    
6. `paper_questions`：paper_id, question_id, score, order
    
7. `exams`（一次发布的考试）：id, paper_id, start_time, end_time, duration_min, published_by
    
8. `attempts`（一次作答）：id, exam_id, student_id, started_at, submitted_at, total_score, status
    
9. `attempt_answers`：attempt_id, question_id, student_answer(json), is_correct, score, feedback(text/json), time_spent_sec
    
10. `llm_logs`：id, user_id, scene(generate/review/grade), model, latency_ms, prompt_tokens, completion_tokens, status, error_msg, created_at
    

### 薄弱点统计怎么做（不需要复杂算法）

- 每个题都绑定知识点 `kp_id`
    
- 统计某学生在某知识点下：
    
    - 总题数、正确题数、得分/总分、平均用时
        
- 排序取最低的 TopN 即“薄弱点”
    

---

## 8. LLM 模型与调用策略（国内模型优先 + 多模型可选）

你要求“尽量国内模型”，建议你后端做一个 **Provider 抽象层**：

- `ModelProvider` 接口：
    
    - `chat(prompt, stream=False) -> text/stream`
        
    - `model_name`
        
    - `api_key`（只在后端环境变量里）
        
    - `base_url`
        

这样你可以做到：

- 默认：用一个主模型（例如 DeepSeek 或 Qwen 或 GLM，你按学校/预算选）
    
- Reviewer：可切换到另一个模型（体现“多模型自审”）
    
- 批改：也可独立选择模型
    

> 论文写作上，这就是“多模型协同的出题与评测流程设计”。

---

## 9. 开发里程碑计划（按 8~10 周节奏，稳稳本科量）

> 你没有进度要求，我按“从现在开始”的标准节奏给你排。你可把每周目标写进开题/进度表。

### 第 1 周：需求冻结 + 环境搭建

- 明确 MVP：题型先做 4 个（单选/多选/填空/简答）
    
- 建仓库：`frontend/` + `backend/`
    
- 后端 FastAPI 跑通；前端能访问后端 `/health`
    
- SQLite 连接 + migration（Alembic 可选，但建议上）
    

**验收**：能本地启动前后端；Swagger 能看到接口。

---

### 第 2 周：用户与角色（系统“骨架”）

- 注册/登录/JWT
    
- 角色权限（教师/学生）
    
- 基础数据：课程、知识点 CRUD
    

**验收**：教师能创建课程和知识点；学生能登录看到自己的主页。

---

### 第 3~4 周：出题流水线（你的“核心创新点”）

- 教师端出题配置表单
    
- 后端 `POST /papers/generate`：
    
    - 调用 LLM 生成 JSON
        
    - Validator 校验
        
    - Reviewer 自审（可先做同模型自审，后续再切双模型）
        
    - 生成通过后入库 `questions` + `paper`
        
- 前端展示生成过程（可做流式）
    

**验收**：老师点一下能生成试卷并看到题目列表（可编辑/通过/驳回）。

---

### 第 5 周：考试发布与作答

- 教师发布考试（设置时长/开始结束时间）
    
- 学生进入考试，获取题目、作答、自动保存
    
- 交卷
    

**验收**：学生能完整走完“开始考试→答题→交卷”。

---

### 第 6 周：自动判分与 AI 批改

- 客观题：直接判分
    
- 简答题：LLM 批改（输出分数 + 建议）
    
- 结果页：每题得分、总分、反馈
    

**验收**：交卷后能看到总分与逐题反馈。

---

### 第 7 周：历史记录 + 错题本 + 薄弱点统计

- 学生历史 attempts 列表
    
- 错题本（按知识点聚合）
    
- 薄弱点 TopN（知识点维度）
    
- 教师端查看班级统计（可先只做“考试分布 + Top 薄弱点”）
    

**验收**：能展示“我最薄弱的知识点是哪些”。

---

### 第 8 周：系统收尾与部署

- LLM 日志落库与管理页（简单列表即可）
    
- Docker compose 一键部署到云服务器
    
- 准备答辩演示数据（预置课程/知识点/考试）
    

**验收**：云端可访问；演示路径不卡。

---

### 第 9~10 周（可选）：打磨 + 写论文 + 做 PPT

- UI 小修、异常处理、超时重试
    
- 论文定稿、PPT、演示彩排
    

---

## 10. 论文写作大纲（你照这个写，内容自然“像毕设”）

> 你不需要做复杂实验分析，那就把重点写在“系统设计 + 关键技术方案 + 运行效果”。

建议章节：

1. 绪论：背景、意义、国内外现状（LLM 在教育测评/出题的应用）
    
2. 需求分析：角色、业务流程、功能/非功能需求
    
3. 总体设计：系统架构、技术选型、数据库设计（ER 图）
    
4. 关键技术与实现
    
    - 题目结构化输出（JSON Schema / Pydantic）
        
    - 题目质量控制流水线（Generator-Validator-Reviewer）
        
    - 自动批改策略（客观题规则 + 主观题 LLM Rubric）
        
    - 薄弱点统计方法（按知识点聚合）
        
5. 系统实现与展示：主要页面、核心接口、部署方式
    
6. 总结与展望：不足（如更强防作弊、RAG、更多题型等）
    

“创新点/特色”你可以这样写（很稳）：

- **提出并实现**：基于大模型的“出题-规则校验-自审复核”三阶段质量控制流程
    
- **实现**：面向教师/学生双角色的在线考试闭环
    
- **实现**：基于知识点的学习统计与薄弱点分析
    

---

## 11. 答辩 PPT 结构（不需要技术文档也能讲清楚）

PPT 建议 12~15 页：

1. 选题背景与意义
    
2. 目标与范围（强调不做本地部署/微调）
    
3. 系统总体架构图（前后端分离 + DB + LLM）
    
4. 角色与业务流程图（教师/学生）
    
5. 数据库 ER 图（核心表）
    
6. 出题质量控制流程（Generator-Validator-Reviewer）
    
7. 批改流程（规则判分 + LLM 批改）
    
8. 薄弱点统计展示（图/榜单）
    
9. 系统页面截图（教师端、学生端、结果页）
    
10. 云端部署图（docker-compose + Nginx）
    
11. 总结与展望
    
12. Q&A
    

---

## 12. 最建议你用的“演示脚本”（答辩现场按这个点就不容易翻车）

**演示角色：一个教师账号 + 一个学生账号**

1. 教师登录 → 选择课程 → 选择知识点（例如“异常/集合/多线程”）→ 设置难度与题型
    
2. 点击“AI 生成试卷”→ 展示流式生成
    
3. 展示“自审结果/规则校验”→ 教师一键通过 → 发布考试（30 分钟）
    
4. 切到学生端 → 开始考试 → 作答 → 交卷
    
5. 展示自动判分 + AI 批改反馈
    
6. 展示“错题本 + 薄弱知识点 Top3”
    
7. 切回教师端 → 查看班级统计（分数分布、薄弱点排行）
    

---

## 13. 风险与降级方案（提前写进论文/进度里，老师会觉得你很稳）

1. **LLM 输出不合规（JSON 乱、字段缺失）**
    
    - 降级：走“格式修复 prompt”一次；还不行标记失败并提示重试
        
2. **题目事实错误/答案不唯一**
    
    - 降级：Reviewer 纠错一次；仍失败则进入“待教师审核”池，不给学生用
        
3. **调用超时/接口不稳定**
    
    - 降级：切备用模型；或提示“稍后重试”；日志记录错误用于复盘
        
4. **API Key 泄露风险**
    
    - 必须：Key 只放后端环境变量；前端永远不直接调用模型厂商接口
        

---

## 14. 你现在马上该做的第一步（不问你问题，直接给行动清单）

按“最快进入开发状态”的顺序：

1. 建一个新仓库（或新目录）做 **monorepo**：`frontend/` + `backend/`
    
2. 把参考项目代码放进 `frontend/`（保留 UI 与答题组件），先跑起来
    
    - 它本身就是 React + TS + Vite + Tailwind 的路线([GitHub](https://github.com/Moyucharm/123 "GitHub - Moyucharm/123"))
        
3. 新建 `backend/`：FastAPI + SQLite + `/health`
    
4. 前端把所有“LLM 直连”改成调用 `/api/llm/*`（先做一个 `/api/generate` 假接口返回 mock）
    
5. 数据库先建 3 张表：`users / courses / knowledge_points`，把登录与知识点管理先跑通
    

做到这一步，你就已经从“课程设计 demo”正式变成“毕设工程”了。

---

如果你愿意，我还可以在你确定“题型先做哪 4 个 + 你打算演示的课程方向（比如 Java / 数据结构 / 高数等）”之后，直接给你一份更落地的：

- **接口清单（按页面对应）**
    
- **数据库建表 SQL（SQLite 版）**
    
- **出题/审题/批改三套 prompt 模板（国内模型通用风格）**
    
- **每周检查点（用于开题/中期检查）**
    

但即使你不补充信息，上面这份规划已经足够你从现在开始按里程碑推进并写论文、做答辩。


# 二、如何 vibe coding
做“vibe coding”（让 AI 帮你快速写代码）最关键不是“让它写”，而是**你把任务切成 AI 一次能稳定交付的小块（ticket / slice）**，每一块都有明确输入输出、验收标准、要改哪些文件、不能做什么。这样 AI 才不会一口气把架构写崩、引入一堆你不想要的依赖，或者做出你没法调试的“魔法”。

下面我给你一套**可直接照抄执行的拆分方法 + 适配你这个毕设的分块清单 + 每块怎么下指令的 Prompt 模板**。

---

## 1）怎么切块：用“三明治切法”

每一块（一个 ticket）最好同时满足：

### A. 纵向切（Vertical Slice）：能从页面走到数据库

不要按“先把所有 DB 写完/先把所有页面写完”这种横向切（很容易接口反复改）。  
而是按**一个用户故事**纵向切，比如：

- “教师能创建课程与知识点”  
    → 前端页面 + 后端接口 + DB 表 + 简单校验
    

### B. 颗粒度要小：一块最多影响 3～8 个文件

AI 一次改太多文件，你 review 成本爆炸，也难定位 bug。  
**经验：一块能在你一次 review 里完全看懂并跑通**。

### C. 每块必须有“验收动作”

例如：

- “调用 `POST /api/auth/login` 返回 JWT”
    
- “前端能保存课程并刷新列表”
    
- “生成的题目能被 validator 校验通过，不通过会返回原因”
    

> 没有验收标准的块，AI 会给你“看起来像做了”的代码，但你跑不通。

---

## 2）你这个毕设：推荐拆成这些块（按开发顺序）

我按你要做的“LLM 出题 + 考试 + 自审 + 统计”的闭环来拆。你可以把它当成 backlog。

### 0. 仓库与运行骨架（必做）

**目标**：前后端都能一键跑起来（本地）。

- `frontend/`：沿用你参考项目（React+TS+Vite）
    
- `backend/`：FastAPI + SQLite（先跑 `/health`）
    
- `docker-compose`（可后置）
    

**验收**：浏览器访问前端；前端能请求后端 `/api/health` 成功。

---

### 1. 用户与角色（Teacher/Student）登录鉴权（必做）

- DB：`users` 表
    
- 后端：注册/登录/JWT、角色中间件
    
- 前端：登录页 + token 存储 + 路由守卫
    

**验收**：教师/学生分别登录后能看到不同菜单。

---

### 2. 课程 + 知识点管理（必做）

- DB：`courses`、`knowledge_points`
    
- 后端：CRUD（只做教师端）
    
- 前端：教师端页面（列表、创建、编辑、删除）
    

**验收**：教师能新增课程、新增知识点，并在 DB 持久化。

---

### 3. LLM 调用封装（必做，但先做最小）

- 后端：`llm_client.py`（统一调用入口）
    
- 仅实现：`chat(prompt) -> text` + 可选 SSE
    
- Key 放环境变量，前端不允许触碰 Key
    

**验收**：后端 `POST /api/llm/test` 返回模型回复。

---

### 4. 题目 Schema + Validator（你的核心之一）

- 定义题目统一结构（Pydantic）：
    
    - 单选/多选/填空/简答 4 类
        
- Validator（代码规则校验）：
    
    - 答案是否在选项内、单选是否唯一、多选是否非空、填空空数匹配、简答有 rubric 等
        
- 错误返回要“可读”（告诉你哪里错）
    

**验收**：传入一个坏题目 JSON，会返回明确错误；好题目通过。

---

### 5. 出题流水线 v1：生成 → 校验 → 入库（先不做自审）

- DB：`questions`、`papers`、`paper_questions`
    
- 后端：`POST /api/papers/generate`  
    输入：课程 + 知识点 + 难度 + 题型 + 数量  
    输出：paper + questions（保存到 DB）
    
- 前端：教师端“生成试卷”页面（能展示题目列表）
    

**验收**：教师点一下生成试卷，刷新后还能看到这份试卷。

---

### 6. 出题流水线 v2：加自审 Reviewer（你的核心之二）

- 后端：`review_question(question)-> {is_correct, issues, fixed_question?}`
    
- 策略：
    
    - Reviewer 不通过：尝试自动修复一次；再不行标记 `status=needs_review`
        
- 前端：教师端“题目审核”页面（通过/驳回/重写本题）
    

**验收**：故意让模型生成一个歧义题，系统能标记出来并要求人工确认。

---

### 7. 考试发布与作答（闭环必做）

- DB：`exams`、`attempts`、`attempt_answers`
    
- 后端：
    
    - 教师发布考试
        
    - 学生开始考试（拉题）
        
    - 保存答案（自动保存）
        
    - 交卷
        
- 前端：学生端考试页（计时、导航、自动保存、交卷）
    

**验收**：学生能完整走完“开始→作答→交卷”。

---

### 8. 判分与 AI 批改（重要展示点）

- 客观题：规则判分
    
- 简答题：LLM 批改（输出分数+反馈）
    
- 结果页：逐题得分 + 总分 + 建议
    

**验收**：交卷后能看到分数和解释；简答题有 AI 反馈。

---

### 9. 历史记录 + 错题本 + 薄弱点统计（你的方向亮点）

- 学生：历史 attempts 列表、错题本
    
- 统计：按知识点正确率/得分率 TopN 薄弱点
    
- 教师：班级/考试统计（最低限度：分数列表 + 薄弱点排行）
    

**验收**：能清楚展示“我薄弱的知识点有哪些”。

---

### 10. 日志与部署（加分项，但很实用）

- `llm_logs`（耗时、模型、场景、成功失败）
    
- Docker compose + Nginx 反代部署到云服务器
    

**验收**：云端能访问；LLM 调用失败能在日志里看到。

---

## 3）每一块让 AI 做什么：用“Ticket 模板”来喂给 AI

你每次找 AI 做一个块时，建议你把信息按这个格式发（复制粘贴就能用）：

### ✅ 通用 Ticket Prompt 模板

你可以直接把下面当成“给 AI 的指令模板”。

```text
你是我的项目协作开发者。请只完成【本 Ticket】范围内的代码改动，不要额外扩展功能。

【项目背景】
- 前端：React + TS + Vite（已有基础项目）
- 后端：FastAPI + SQLite
- 前后端分离，LLM 只能在后端调用（前端不允许出现 API Key）
- 题目要结构化 JSON（Pydantic 校验）

【本 Ticket 标题】
（写一句话）

【目标/用户故事】
（写清楚用户能做什么）

【接口契约】
- 新增/修改哪些 API（method + path + request/response 示例）
- 权限要求（teacher / student）

【数据模型】
- 需要哪些表/字段（或说明不涉及 DB）

【验收标准】
1) ...
2) ...
3) ...

【约束】
- 不引入新框架（除非我明确允许）
- 代码风格：...
- 变更尽量小，优先复用已有文件

【当前仓库结构（我会贴 tree）】
（贴出关键目录结构）

【请你输出】
1) 需要新增/修改的文件清单
2) 每个文件的完整代码（或最小 diff）
3) 我应该如何本地运行/验证（命令 + 操作步骤）
```

> 你会发现：只要你把“接口契约 + 验收标准”写清楚，AI 就很少跑偏。

---

## 4）把一个 Ticket 再切成“AI 更好写”的子任务（超实用）

即使是一个 ticket，也建议你用 **“先合同、后实现”** 的顺序让 AI 做：

### 子任务 1：先定接口与数据结构（不写大段代码）

让 AI 输出：

- endpoint 列表
    
- request/response JSON 示例
    
- Pydantic models 草案
    
- DB 表设计草案
    

你 review 一次，确认“不会反复返工”。

### 子任务 2：后端实现（只改后端）

让 AI：

- 写模型、CRUD、路由、权限校验
    
- 附带最小的测试（哪怕是 curl 命令也行）
    

### 子任务 3：前端对接（只改前端）

让 AI：

- 写 api client（fetch 封装）
    
- 写一个页面组件完成最小闭环
    
- 不要顺手重构整个前端
    

### 子任务 4：联调修 bug（把报错原样贴回去）

你把：

- 后端报错堆栈
    
- 前端 console error
    
- 请求/响应 body  
    贴给 AI，让它“只修 bug，不加功能”。
    

---

## 5）给你几个“你这个项目最常用”的具体 Prompt 样例

### 样例 A：做“题目 Schema + Validator”这个核心块

```text
本 Ticket：实现 Question 的 Pydantic Schema 和 validator 规则校验。

要求：
- 支持单选、多选、填空、简答四类题
- 统一输出结构：type, stem, options?, answer, explanation, knowledge_point_id, difficulty(1-5), score
- 单选：answer 是单个选项 key；多选：answer 是 key 数组；填空：answer 是字符串数组；简答：answer 是评分要点数组（rubric）
- validator 要返回可读错误：指出是哪一条规则、哪一个字段失败
- 写一个 /api/debug/validate_question 接口：传入 question JSON，返回 {ok: true} 或 {ok:false, errors:[...]}
- SQLite 不需要改表（这块只做校验）

请输出：文件清单 + 完整代码 + curl 验证命令。
```

### 样例 B：做“生成试卷（不自审版）”

```text
本 Ticket：实现教师生成试卷接口 /api/papers/generate（不做 reviewer）。

输入：
{
  "course_id": 1,
  "knowledge_point_ids": [1,2,3],
  "difficulty": 3,
  "counts": {"single":5,"multiple":3,"blank":2,"short":1}
}

行为：
- 后端调用 llm_client 生成题目 JSON 数组（按我们已有 schema）
- validator 校验，不通过则返回 errors
- 通过则入库 questions / papers / paper_questions
- 返回 paper_id 和 questions 简要信息

要求：只允许 teacher 调用；LLM key 只在后端 env。

请输出：文件清单 + 完整代码 + 运行验证步骤。
```

### 样例 C：做“自审 Reviewer（用同模型也行）”

```text
本 Ticket：实现 reviewer：对生成题目做正确性与歧义检查。

要求：
- 新增 review_question(question)->ReviewResult(is_correct, issues[], fixed_question?)
- issues 分类：事实错误/答案不唯一/表述歧义/难度不符/解析不充分/选项设置问题
- 先尝试让 reviewer 给 fixed_question；若 fixed_question 仍通不过 validator，则标记 needs_review
- questions 表增加 status: approved / needs_review
- 前端教师页显示 needs_review 列表，并提供“通过/驳回/重新生成本题”

请输出：后端改动 + 前端最小 UI（列表+按钮）+ 验证步骤。
```

---

## 6）Vibe coding 的“防翻车”规则（强烈建议你照做）

1. **永远不要让 AI 一次“从零做完整系统”**  
    你会得到一个看似完整、但不可维护不可调试的项目。
    
2. **每个 Ticket 都要写“非目标 Non-goals”**  
    例如：“不做班级管理、不做导出、不做防作弊”。
    
3. **每次让 AI 输出“文件清单 + 完整代码/最小 diff”**  
    不要只要“思路”。你要的是可运行的增量提交。
    
4. **出错时把日志原样贴回去**  
    AI 修 bug 的成功率取决于你给它的报错质量。
    
5. **每块完成立刻打一个 commit**  
    你的心理安全感来自于“随时可回滚”。
    

---

## 7）如果你愿意，我可以直接带你用“Ticket 驱动”推进

你只要按下面格式回复我一次，我就能从第 0 块开始，按块给你逐个产出代码（每次一块，不会失控）：

- 你当前前端项目的目录结构（`tree -L 2` 输出即可）
    
- 你想用的后端语言：Python FastAPI（我默认这个）
    
- 你希望先做的第一个块：  
    A) 后端骨架 + /health  
    B) 登录鉴权  
    C) 课程/知识点 CRUD
    

你不需要一次性想清楚全部，我会按“最小闭环”的顺序拆给你。